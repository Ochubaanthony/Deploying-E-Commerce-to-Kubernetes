# Deploying-E-Commerce-to-Kubernetes
Step by steps to deployed Project on Kubernetes


HOW TO CONNECT TO 1 or Many KUBERNETES CLUSTERS from Command line?

On the server where you run 

git clone  https://github.com/iam-veeramalla/ultimate-devops-project-demo
cd ultimate-devops-project-demo





Overview 
n the previous session, we learned how to connect to an EKS cluster and explored kubectl. In this lecture, the focus is on Kubernetes implementation for the project, covering the following key points:
	â€¢	Introduction to Kubernetes concepts like service accounts, deployments, and services.
	â€¢	Explanation of why service accounts are needed in real-world deployments.
	â€¢	Understanding Kubernetes deployments and services to grasp scaling and service discovery.
	â€¢	Project deployment to Kubernetes using predefined YAML files for each microservice (available in the GitHub repo ultimate-devops-project-demo).
	â€¢	Overview of the deployment and service files for various microservices.
	â€¢	Learning how to expose the application to the external world using:
	â€¢	LoadBalancer service
	â€¢	Ingress and Ingress Controller
	â€¢	Detailed steps provided for setting up ingress and accessing the project externally.
	â€¢	Demonstration of using a custom domain with AWS Route 53, integrating it with Ingress for real-world scenarios.
	â€¢	End-to-end implementation ensures a comprehensive understanding of Kubernetes in a real project.
	â€¢	Upcoming in the CI/CD section: Deploying the app to Kubernetes using Argo CD (part of CD, not this section).
	â€¢	Beginners are encouraged to watch the Kubernetes Zero to Hero and Docker series for foundational understanding.

Conclusion: This lecture sets up a full Kubernetes deployment pipeline with real-world practices and prepares you for interviews.



Class 2
Kubernetes server Account and its importance in realtime


When deploying demo applications as pods in Kubernetes, many skip creating a service account, and Kubernetes still allows the pod to run. This is because Kubernetes automatically assigns a default service account (default) to every pod if no specific one is provided.

Key Concepts:
	â€¢	User Account vs. Service Account:
	â€¢	User Account: For humans (e.g., DevOps, developers) to access the cluster using kubectl and kubeconfig.
	â€¢	Service Account: For services (like pods) to interact with Kubernetes resources.
	â€¢	Why Service Accounts Matter:
	â€¢	Just like users need permissions, pods also need permissions to interact with the Kubernetes API.
	â€¢	The default service account provides minimal permissions (enough to run the pod).
	â€¢	For advanced actions (e.g., accessing ConfigMaps, calling the Kubernetes API), the pod must be assigned a custom service account with specific permissions.

Permissions and Role Binding:
	â€¢	To grant additional permissions:
	â€¢	Create a Role or ClusterRole.
	â€¢	Bind it to the service account using RoleBinding or ClusterRoleBinding.
	â€¢	This is similar to AWS IAM:
	â€¢	Service accounts are like IAM roles.
	â€¢	Permissions are defined in roles (like IAM policies).
	â€¢	Roles are linked to service accounts through bindings.

Important Notes:
	â€¢	In this project, custom roles or bindings are not required, as the microservices are independent and do not need to access the Kubernetes API.
	â€¢	For more advanced Kubernetes development (e.g., controllers, webhooks), using a custom service account with proper permissions is essential.

â¸»

Conclusion:
Even though the default service account is sufficient for simple applications, in real-world environments and production deployments, itâ€™s important to create and assign custom service accounts with appropriate permissions to ensure secure and functional Kubernetes operations.

Class 3
Kubernetes Deployment -[scaling and Healing]

Hereâ€™s a clear and concise summary of the lecture on Kubernetes Deployments:

â¸»

ğŸ” Overview

Before deploying the project on EKS, itâ€™s essential to understand two core Kubernetes resources:
	1.	Deployment â€“ handles scaling and auto-healing.
	2.	Service â€“ manages service discovery (covered in the next lecture).

â¸»

âš ï¸ Problem with Plain Containers
	â€¢	Containers (via Docker/Podman) are ephemeral â€” they donâ€™t restart automatically if they crash.
	â€¢	IP addresses change on restarts, making networking and service communication unreliable.
	â€¢	During high traffic (e.g., a sales event), a single container canâ€™t handle the load; we need multiple copies and load balancing.

â¸»

ğŸš€ Solution: Kubernetes Deployment

The Deployment resource solves this by enabling:
	â€¢	Scaling: Set the number of pod replicas (e.g., 3, 4, 10, 100) to handle more traffic.
	â€¢	Auto-healing: If a pod crashes, Kubernetes automatically replaces it to maintain the desired replica count.

ğŸ”§ How it works:
	â€¢	You create a Deployment YAML with a specified replicas count.
	â€¢	Kubernetes creates a ReplicaSet (a controller) behind the scenes.
	â€¢	The ReplicaSet maintains the desired number of Pods.
	â€¢	If a pod fails, it immediately creates a new one, ensuring uptime.

â¸»

ğŸ’¡ Key Takeaways:
	â€¢	Pod: The basic unit in Kubernetes (holds one or more containers).
	â€¢	ReplicaSet: Ensures the desired number of pod replicas are running.
	â€¢	Deployment: Manages the ReplicaSet and provides scaling + healing features.
	â€¢	Why itâ€™s better than Docker alone: You get resilience and scalability out-of-the-box.

â¸»

ğŸ”œ Next Topic:

In the upcoming lecture, youâ€™ll learn about the Service resource, which enables service discoveryâ€”essential for microservices to find and communicate with each other.

Let me know if youâ€™d like a YAML template or diagram for Deployments!



Class 4
Kubernetes service [service discovery]

Hereâ€™s a clear and concise summary of the lecture on Kubernetes Services:

â¸»

ğŸ” Recap from Last Lecture
	â€¢	Deployment resource handles scaling and auto-healing.

â¸»

ğŸŒ Current Topic: Kubernetes Service
	â€¢	Problem Solved: Service Discovery
	â€¢	In Docker-only setups, containers get new IPs when restarted.
	â€¢	If front-end relies on a fixed back-end IP, communication breaks when back-end IP changes.
	â€¢	Manual updates and restarts are error-prone and inefficient.

â¸»

ğŸš€ How Kubernetes Solves This
	â€¢	Kubernetes introduces a Service resource to solve the service discovery issue.

ğŸ”§ How it works:
	1.	Deployments create ReplicaSets, which in turn create Pods (front-end and back-end).
	2.	Each Pod gets a label (like a tag/ID).
	3.	Service acts as a stable proxy or load balancer between Pods.

ğŸ”‘ The service does not use IP addresses to locate Pods. It uses labels.

	â€¢	When a back-end Pod crashes and is replaced (with a new IP), the label remains the same.
	â€¢	The Service watches the label and redirects traffic to the new Pod automatically.

â¸»

ğŸ’¡ Key Takeaways:
	â€¢	Front-end talks to the Service, not directly to the back-end Pod.
	â€¢	The front-endâ€™s environment variable points to the Service name, not an IP.
	â€¢	The Service then finds and routes traffic to the correct Pod using label selectors.
	â€¢	This ensures reliable communication even if Pods restart or scale.

â¸»

ğŸ”— Comparison:

Feature	Docker-Only Setup	Kubernetes with Services
Pod/Container IP	Changes on restart	Irrelevant (label-based routing)
Communication	IP-dependent, fragile	Label-based, stable
Recovery from Crash	Manual fix/update needed	Handled automatically by Service


â¸»

ğŸ§  Summary:
	â€¢	Deployment solves: Scaling & Auto-Healing
	â€¢	Service solves: Service Discovery & Reliable Communication
Together, they make Kubernetes far more robust than standalone container tools.

â¸»




Class 5
Deploying the project on kubernetes -Part1

Hereâ€™s a clear and structured summary of the lecture on Deploying Microservices in Kubernetes Using Deployment, Service, and ServiceAccount:

â¸»

ğŸ“ Recap of Previous Lectures
	â€¢	Learned about:
	â€¢	Deployment: Manages replicas and ensures high availability.
	â€¢	Service: Solves the problem of service discovery.
	â€¢	ServiceAccount: Provides access control and identity to pods.

â¸»

ğŸš€ Current Focus: Deploying a Real Project on EKS
	â€¢	The project consists of ~20 microservices.
	â€¢	Creating YAML files for each manually would take hours.
	â€¢	Good news: All manifests are already created and available in the GitHub repo ultimate-devops-project-demo.

â¸»

ğŸ“ Manifest Structure in the GitHub Repo
	1.	Folder per Microservice:
	â€¢	Example: add, checkout, etc.
	â€¢	Each contains its own:
	â€¢	deployment.yaml
	â€¢	service.yaml
	2.	Single Combined File:
	â€¢	complete-deploy.yaml (~1907 lines).
	â€¢	Includes manifests for all microservices.
	â€¢	Separated by --- (YAML multi-document format).

You can choose:
	â€¢	kubectl apply -f complete-deploy.yaml to deploy everything at once.
	â€¢	Or apply microservice by microservice for better understanding.

â¸»

ğŸ“„ Structure of a Deployment YAML
	â€¢	Common Fields Across All Resources:
	â€¢	apiVersion
	â€¢	kind
	â€¢	metadata: includes name, labels, and annotations
	â€¢	Spec Section (specific to resource type):
	â€¢	For Deployment:
	â€¢	replicas: number of pod replicas (managed by ReplicaSet)
	â€¢	template:
	â€¢	metadata: includes pod-level labels (used by Services)
	â€¢	spec:
	â€¢	containers: container settings (image, ports, env vars, etc.)

ğŸ§  Tip: The template inside spec defines the Pod templateâ€”like Docker Compose in structure.
Labels here are critical for service discovery.

â¸»

ğŸ·ï¸ Labels for Service Discovery
	â€¢	Example label for the pod:

app.kubernetes.io/name: add-microservice


	â€¢	This is how the Service resource locates the correct pod, even if its IP changes.

â¸»

âœ… Key Takeaways

Concept	Purpose
Deployment	Ensures scaling and availability via ReplicaSet
Service	Provides stable access to pods via labels
ServiceAccount	Defines identity/access for pods
Metadata & Labels	Used for both identification and routing


â¸»

ğŸ§ª Whatâ€™s Next
	â€¢	In the following steps/lectures, youâ€™ll:
	â€¢	Examine one microserviceâ€™s YAML in detail.
	â€¢	Understand how to write deployment.yaml and service.yaml.
	â€¢	Deploy on EKS using either full or microservice-by-microservice approach.

ğŸ¯ Objective

Deepen your understanding of how to define a complete Kubernetes Deployment manifest â€” especially focusing on pod-level configuration, including containers, service accounts, ports, environment variables, and more.

â¸»

ğŸ§± Core Structure of a Kubernetes Deployment Manifest

âœ… 1. Top-Level Fields (Standard for all K8s resources):

apiVersion: apps/v1
kind: Deployment
metadata:
  name: <deployment-name>
  labels:
    <key>: <value>

	â€¢	apiVersion, kind, metadata â€” common to all K8s resources.
	â€¢	Labels here are NOT used for service discovery â€” only for organization & identification.

â¸»

âœ… 2. Spec Section (Deployment-Level)

spec:
  replicas: <number>
  template:
    metadata:
      labels:
        <key>: <value>
    spec:
      serviceAccountName: <account-name>
      containers:
      - name: <container-name>
        image: <image>
        ports:
        - containerPort: <port>
        env:
        - name: <env-name>
          value: <env-value>

	â€¢	replicas: Number of pod replicas.
	â€¢	template: Defines pod configuration.
	â€¢	metadata.labels: Used by Service to find the pod (key for service discovery).
	â€¢	spec (inside template): Core pod configuration:
	â€¢	serviceAccountName: Custom SA or defaults to default if unspecified.
	â€¢	containers: A list (start with -) of container specs (name, image, ports, env vars, etc.).

â¸»

ğŸ§  Key Learnings & Best Practices

Concept	Summary
ServiceAccount	Must be explicitly defined to override default.
Container Definition	Must include name, image, port, and optional env vars, volumes, and resource limits.
Image Sources	Though only a few microservices were containerized in the lectures, the GitHub repo already includes correct image names for all services (from Docker Hub or GHCR).
Environment Variables	Often look intimidating â€” but are usually provided by developers (e.g., for service URLs, credentials, etc.).
Similarity to Docker Compose	Structure is almost identical (name, image, ports, env, volumes). Writing manifests becomes easier with experience.


â¸»

ğŸ›‘ Common Misconceptions
	â€¢	â€œItâ€™s too long!â€: A 60-line YAML doesnâ€™t mean you have to manually write 60 lines â€” you copy, tweak, and reuse most parts.
	â€¢	â€œI must know all env vars!â€: No â€” youâ€™re responsible for the structure. Dev teams provide environment-specific details.

â¸»

ğŸ“Œ Pro Tips
	â€¢	Understand the skeleton:
	â€¢	apiVersion, kind, metadata, spec
	â€¢	Within spec: replicas, template, and inside template.spec: pod-level config
	â€¢	Focus on the fields that vary:
	â€¢	Image name
	â€¢	Ports
	â€¢	Env vars
	â€¢	Volumes
	â€¢	Refer to existing YAMLs and the Kubernetes documentation for field definitions.

â¸»

ğŸ§ª What Should You Do Now?
	â€¢	Practice by reading deployment YAMLs for different microservices (e.g., currency, add, etc.).
	â€¢	Recognize patterns â€” the structure is consistent.
	â€¢	Experiment by editing values (e.g., change replicas, image versions) and apply them on a test cluster.

â¸»



Class 6
Deploying the project on Kubernetes -Part2


ğŸ¯ Objective

Understand the structure of a Kubernetes Service resource and how it connects to pods defined in a Deployment.

â¸»

ğŸ§± Core Structure of a Kubernetes Service Manifest

âœ… 1. Top-Level Fields

apiVersion: v1
kind: Service
metadata:
  name: <service-name>
  labels:
    <key>: <value>

	â€¢	apiVersion: Always v1 for services.
	â€¢	kind: Always Service.
	â€¢	metadata: Includes name and optional labels for identification and troubleshooting.

â¸»

âœ… 2. Spec Section (Service-Level Configuration)

spec:
  type: ClusterIP | NodePort | LoadBalancer
  ports:
  - port: <service-port>
    targetPort: <container-port>
  selector:
    <label-key>: <label-value>

Field	Meaning
type	How the service is exposed (internal or external):ClusterIP (default, internal use), NodePort, LoadBalancer
port	The port on which the service will be available (can be arbitrary)
targetPort	The containerâ€™s port in the pod to forward requests to (must match podâ€™s config)
selector	The label the service uses to find the target pod(s)


â¸»

ğŸ”„ How Services Link to Deployments
	â€¢	Service uses selector to find pods.
	â€¢	The label in the selector must match a label defined in the pod template inside the deployment.
	â€¢	The target port must match the container port defined in the deployment.

ğŸ“Œ Example Connection:
	â€¢	Deployment YAML:

spec:
  template:
    metadata:
      labels:
        app: currency-service
    spec:
      containers:
      - name: currency
        image: currency:v1
        ports:
        - containerPort: 8080


	â€¢	Service YAML:

spec:
  type: ClusterIP
  selector:
    app: currency-service
  ports:
  - port: 8080
    targetPort: 8080



â¸»

ğŸ§  Key Takeaways

Concept	Summary
Selector	The critical bridge â€” helps the service know which pod(s) to forward traffic to.
Port vs TargetPort	port is the exposed port by the service; targetPort is the port inside the pod (must match container definition).
Service Name as FQDN	Other microservices talk to it via: service-name.namespace.svc.cluster.local:<port>
Match Labels Carefully	If selector labels donâ€™t match pod labels, the service wonâ€™t work.
Documentation Is Key	Kubernetes docs are your friend â€” nobody memorizes everything at first. You get better with practice.


â¸»

ğŸ“š Assignment

Explore the Kubernetes manifests in your project:
	1.	For each microservice, open both:
	â€¢	deployment.yaml
	â€¢	service.yaml
	2.	Identify:
	â€¢	The labels on the deploymentâ€™s pod template
	â€¢	The selector in the service
	â€¢	Match the targetPort with the containerPort
	3.	Understand how each service is tied to its respective pod.

â¸»

ğŸ§ª Next Steps (Teaser for Next Lecture)

Youâ€™ll soon:
	â€¢	Deploy your microservices using kubectl apply -f <manifest>.
	â€¢	Observe whether resources are created successfully.
	â€¢	Learn basic debugging if things go wrong.

â¸»

Class 7
Deploying the project on Kubernetes -Part 3


ğŸ¯ Objective

Understand why deployed services arenâ€™t accessible externally yet, and how Kubernetes service types and networking affect that accessibility.

â¸»

ğŸ”— Internal Service Communication Recap
	â€¢	In Kubernetes, services communicate using internal DNS-based service discovery.
	â€¢	Example interview Q&A:
Q: How does one microservice connect to another?
A: For example, the shipping service connects to the code service using environment variables that store the service name of the code service. Kubernetes DNS resolves this name internally.
	â€¢	Service Discovery via Environment Variables is a common pattern (used instead of or alongside ConfigMaps).

â¸»

ğŸš« Why You Canâ€™t Access the Project Yet

ğŸ§ª Attempt:
	â€¢	You run:

kubectl get svc | grep frontend-proxy


	â€¢	You find an IP like 172.x.x.x and port 8080.
	â€¢	You try to open it in your browser using:

http://172.x.x.x:8080



âŒ Result:
	â€¢	You canâ€™t access it. Why?

â¸»

ğŸ§± Understanding the Network Setup
	â€¢	Your EKS cluster is inside a VPC created using Terraform.
	â€¢	It contains both private and public subnets.
	â€¢	Your Kubernetes services (e.g., frontend-proxy) are currently exposed using the ClusterIP type.
	â€¢	ClusterIP is only accessible inside the cluster.
	â€¢	Youâ€™re outside the VPC, so you canâ€™t reach these services directly from your browser.

â¸»

âš™ï¸ Whatâ€™s Missing?

To access your services externally, you need to expose them through a service type that allows external access.

â¸»

ğŸ“Œ Preview of the Next Step

You will:
	â€¢	Learn how to change the service type from ClusterIP to:
	â€¢	NodePort (basic external access through worker node IP and port)
	â€¢	or LoadBalancer (provision a cloud load balancer with a public IP)

This allows the frontend (or frontend-proxy) to be accessible from the public internet.

â¸»

ğŸ§  Key Takeaways

Concept	Summary
Internal Communication	Done via service names in env variables (Kubernetes service discovery)
ClusterIP Limitation	Only works within the cluster (no external access)
VPC Isolation	Your services are in a private network â€“ browser access wonâ€™t work unless exposed
Next Step	Learn about and apply the correct service type to expose frontend



Class 8
Kubernetes Service Types

Hereâ€™s a clear and organized summary of the lecture focused on Kubernetes service types and external access, especially using NodePort:

â¸»

ğŸ¯ Objective

Understand why the frontend is not accessible externally after deployment, and how to expose services using different Kubernetes service types â€” especially the NodePort type.

â¸»

ğŸ” Problem Recap
	â€¢	You deployed the project and tried to access the frontend via service IP + port.
	â€¢	It didnâ€™t work because:
	â€¢	The service is using ClusterIP (default service type).
	â€¢	ClusterIP is only accessible inside the Kubernetes cluster.
	â€¢	The frontend cannot be accessed from outside the cluster â€” not even by resources within the same VPC (like EC2 or Lambda).

â¸»

ğŸ“¦ Kubernetes Service Types Overview

Type	Access Scope	Use Case
ClusterIP	Internal cluster only	Pod-to-pod or service-to-service (e.g., databases)
NodePort	External access via node IP	Allow access within VPC or from external systems via EC2 public IP
LoadBalancer	Public internet via cloud load balancer	Best for production-ready external services


â¸»

ğŸ§  ClusterIP - Default Behavior
	â€¢	Creates a service that is only reachable from within the Kubernetes cluster.
	â€¢	Useful for internal services with sensitive data.
	â€¢	Even other AWS resources inside the same VPC (like EC2 or Lambda) cannot access services with ClusterIP directly.

â¸»

ğŸŒ NodePort - Opening Access via Node
	â€¢	By changing service type to NodePort, Kubernetes:
	â€¢	Allocates a port (e.g., 30000â€“32767) on each nodeâ€™s IP address.
	â€¢	Updates internal iptables via kube-proxy to route traffic from that node port to the service/pod.
	â€¢	Now, resources inside the VPC (like EC2) can access the service using:

<Node IP>:<NodePort>


	â€¢	Examples:
	â€¢	Frontend service becomes available at NodeIP:33000
	â€¢	Product catalog becomes available at NodeIP:34000
	â€¢	Cart service at NodeIP:36000
	â€¢	This allows limited external access, without using a load balancer.

â¸»

ğŸ§± Why This Works
	â€¢	Every node in a Kubernetes cluster is essentially an EC2 instance.
	â€¢	When a service is exposed using NodePort, it can be accessed using the public or private IP of the node â€” depending on how your networking is configured.

â¸»

ğŸ” Important Notes
	â€¢	NodePort opens ports on all nodes in the cluster, regardless of where the pod actually runs.
	â€¢	Good for development or internal access, but not ideal for production due to:
	â€¢	Limited range of ports
	â€¢	No load balancing across nodes
	â€¢	Potential security risks

â¸»

ğŸ“Œ Next Step

You will learn how to use the LoadBalancer service type, which is more scalable and secure for production, and allows access via a cloud-managed public IP or DNS.

â¸»



ğŸŒ Objective

Enable external access (from the public internet) to a service running inside a Kubernetes cluster by using the LoadBalancer service type.

â¸»

ğŸ§± Context Recap
	â€¢	So far, youâ€™ve seen:
	â€¢	ClusterIP â†’ internal-only access (within cluster)
	â€¢	NodePort â†’ access via Node IP (within VPC or organization)
	â€¢	But what if you want public access, i.e., anyone on the internet should reach your service?

â¸»

âš™ï¸ LoadBalancer Service Type
	â€¢	Set type: LoadBalancer in your Service.yaml.
	â€¢	Kubernetes understands this as: â€œI want external access.â€

âœ… What Happens Internally:
	1.	API Server detects the change to LoadBalancer.
	2.	It calls a component called CCM (Cloud Controller Manager).
	3.	CCM communicates with your cloud provider (e.g., AWS, Azure, GCP).
	4.	The cloud provider creates a Load Balancer (e.g., an AWS ELB).
	5.	An external IP or FQDN is assigned and attached to your Kubernetes service.
	6.	This IP/DNS can be accessed from anywhere on the internet.

â¸»

â˜ï¸ Cloud Provider Dependency
	â€¢	Works only on cloud platforms that support CCM.
	â€¢	Examples:
	â€¢	âœ… AWS (EKS), Azure (AKS), GCP (GKE)
	â€¢	âŒ Minikube, Kind, or K3s (local clusters) â†’ Do not support CCM
	â€¢	Changing to LoadBalancer in these cases does nothing

â¸»

ğŸ› ï¸ Summary of Service Types

Service Type	Access Level	Use Case Example
ClusterIP	Only inside Kubernetes cluster	Internal services (e.g., databases, APIs)
NodePort	Inside the VPC or organization network	Dev/test services, internal apps
LoadBalancer	Public internet access via cloud LB	Production apps exposed to the world


â¸»

ğŸ”œ Next Step



Class 9
Accessing the project using Kubernetes LB Service Type

Terminal
ubuntu@ip-172-31-38-42:~/ultimate-devops-project-demo/kubernetes$ kubectl get svc |grep frontendproxy

kubectl edit svc opentelemetry-demo-frontendproxy              			Typy = â€œchange clusterIPâ€>  to>  LoadBalancer
Save with this
esc  :wq!   Enter.                â€œ wait for 5 minsâ€

BUT IF RUN THIS CODE IMMEDATILY YOU SAVE THE CODE
kubectl get svc  opentelemetry-demo-frontendproxy

NOTE TO CONFIRM YOU HAVE LOAD BALANCER ON EC2 INSTANCE
Go to aws console
Ec2 instance
Load balancer of the same region us-west-2 					â€œ you see itâ€
Copy DNS PASTER ON BROSWER
 a242bc24031814f41825f3c38b22c152-993543253.us-west-2.elb.amazonaws.com:8080



Class 10
Disadvantage of LoadBalancer service type


ğŸš« Drawbacks of Using LoadBalancer Service Type in Kubernetes

1. âŒ Lack of Declarative Configuration
	â€¢	LoadBalancer is created automatically via CCM (Cloud Controller Manager).
	â€¢	You cannot configure things like:
	â€¢	TLS certificates (HTTPS)
	â€¢	Load balancing algorithm
	â€¢	Health checks, security settings, etc.
	â€¢	Any modifications (e.g., adding HTTPS via ACM) must be done manually in the cloud providerâ€™s console, not through code (YAML).
	â€¢	This breaks Infrastructure-as-Code (IaC) best practices â€” changes are not trackable in Git or version-controlled systems.

â¸»

2. ğŸ’¸ Not Cost-Effective
	â€¢	If you expose 10 services using LoadBalancer, Kubernetes will create 10 individual cloud load balancers (e.g., ALBs).
	â€¢	Load balancers on AWS are expensive.
	â€¢	Thereâ€™s no built-in support for sharing one load balancer across multiple services with different routes or paths (e.g., /api, /auth, /frontend).

â¸»

3. ğŸ”’ Vendor Lock-in / Lack of Flexibility
	â€¢	Youâ€™re tied to the cloud providerâ€™s default load balancer, e.g.:
	â€¢	AWS â ALB
	â€¢	Azure â Azure Load Balancer
	â€¢	You cannot use alternative solutions like:
	â€¢	NGINX Ingress Controller
	â€¢	Traefik
	â€¢	F5
	â€¢	Youâ€™re limited to what CCM supports.

â¸»

4. ğŸš« Doesnâ€™t Work Without CCM
	â€¢	In local clusters (e.g., Minikube, Kind, K3s), where Cloud Controller Manager is absent, setting type: LoadBalancer does nothing.
	â€¢	No external IP is created, and the service remains inaccessible from outside.

â¸»

âœ… Why Ingress Is Better

Feature	LoadBalancer	Ingress
Declarative Configuration	âŒ Manual edits required	âœ… Fully declarative (YAML)
Cost Efficient	âŒ Multiple LBs needed	âœ… One LB for many services
TLS/HTTPS Support	âŒ Manual via cloud UI	âœ… Via Ingress annotations
Flexibility	âŒ Only cloud LB allowed	âœ… Use NGINX, Traefik, etc.
Works Locally	âŒ No CCM, no effect	âœ… Works with local setups


â¸»

ğŸ§­ Conclusion

Although LoadBalancer type allows public access, it is:
	â€¢	Hard to maintain
	â€¢	Expensive at scale
	â€¢	Not declarative
	â€¢	Tied to your cloud provider

Ingress is the recommended, scalable, and declarative solution for managing external access to services in Kubernetes.





âœ… Why Ingress is Better than LoadBalancer

Pros of Ingress & Ingress Controller:
	1.	Highly Flexible:
	â€¢	You can choose your own load balancer (e.g., NGINX, Traefik, F5, Envoy).
	â€¢	Just deploy the corresponding Ingress Controller.
	2.	Cloud-Independent:
	â€¢	Ingress does not require Cloud Controller Manager (CCM).
	â€¢	Works on Minikube, K3s, Kind, and other non-cloud setups.
	â€¢	Can expose services using reverse proxy mechanisms even without a cloud provider.
	3.	Declarative and Scalable:
	â€¢	All configurations are done using YAML files.
	â€¢	Easy to maintain, version control, and audit.
	â€¢	Can handle multiple routes and services under one external IP.

â¸»

âŒ Cons of LoadBalancer Service Type (Recap):
	â€¢	Not declarative (requires manual changes in cloud console).
	â€¢	Expensive if you expose many services (1 load balancer per service).
	â€¢	Tied to the cloud providerâ€™s default LB (e.g., only ALB on AWS).
	â€¢	Doesnâ€™t work in environments without CCM.

â¸»

âœ… Only Advantage of LoadBalancer Service Type:
	â€¢	Simplicity:
	â€¢	Extremely easy to configure.
	â€¢	No need to deploy additional components like an Ingress Controller.
	â€¢	Just set type: LoadBalancer in the service YAML and it works (in supported environments).

ğŸ”¸ Ideal for quick testing, prototypes, or small-scale deployments.

â¸»

ğŸ§  Takeaway for Interviews:

If asked whether LoadBalancer service type has any benefits:

â€œYes, itâ€™s very easy to set up and doesnâ€™t require additional resources like an Ingress Controller. However, it lacks flexibility, is not declarative, and can be expensive at scale. Thatâ€™s why Ingress is the preferred approach in production.â€



Class 11
Understanding Ingress and Ingress Controllers Part 1
Validate this run

nslookup amazon.com
 https://54.239.28.85						Ubable to view the url do to security

nslookup a242bc24031814f41825f3c38b22c152-993543253.us-west-2.elb.amazonaws.com

COPY THE IP Address and add port 8080
34.214.57.106:8080						able to view the URL which is RED FLAG




Hereâ€™s a summary of this lecture on Ingress and Ingress Controllers in Kubernetes:

â¸»

ğŸ” Recap:
	â€¢	Previously, we accessed a deployed project via a LoadBalancer service type.
	â€¢	We discussed the drawbacks of using that approach.

â¸»

ğŸ“¥ What is Ingress?
	â€¢	In networking, Ingress refers to incoming traffic.
	â€¢	In Kubernetes, Ingress is a Kubernetes resource (like Deployment or Service).
	â€¢	It allows you to define routing rules for incoming traffic to your services or applications.

â¸»

ğŸ“¦ Why Ingress Is Useful:
	â€¢	When a frontend service is deployed in an EKS cluster, by default itâ€™s ClusterIP (accessible only within the cluster).
	â€¢	To make it accessible externally, we used a LoadBalancer, which:
	â€¢	Routes traffic from the internet to the service.
	â€¢	Gets a DNS name from AWS (via Cloud Controller Manager).
	â€¢	Connects to the frontend service through private subnet and exposes it via public subnet.

â¸»

ğŸš« Problem with This Approach:
	â€¢	The AWS LoadBalancer gives a random DNS name (e.g., ab123456.elb.amazonaws.com).
	â€¢	If someone does a DNS lookup (nslookup) on this domain, they get the IP address.
	â€¢	Typing the IP address directly in the browser still allows access to the app (http://<IP>:8080).
	â€¢	Security Issue: This is not acceptable in real-world companies.
	â€¢	Companies want users to access their services only via their domain name (e.g., amazon.com), not via IP.

â¸»

âœ… Solution: Use Ingress
	â€¢	To define strict routing rules (e.g., allow access only via a specific domain), use Ingress.
	â€¢	Ingress lets you:
	â€¢	Specify hostnames (e.g., shop.mycompany.com)
	â€¢	Route traffic based on path or host
	â€¢	Improve security and control over traffic.

â¸»

ğŸ§  Takeaway:

â€œIngress is the right way to control and route incoming traffic in Kubernetes. It allows organizations to enforce domain-based access and avoid exposing their applications via raw IP addresses, which is a security risk.â€



ğŸ”„ Recap:
	â€¢	Ingress is required when you want to define routing rules for incoming traffic.
	â€¢	Kubernetesâ€™ Service resources (like LoadBalancer) cannot define detailed routing rules like host/path-based access.

â¸»

âš™ï¸ How It Works:

ğŸ”¹ Service of Type LoadBalancer:
	â€¢	You define the service with type LoadBalancer.
	â€¢	CCM (Cloud Controller Manager) creates a load balancer.
	â€¢	Simple to set up, but lacks routing control (e.g., cannot restrict access by domain or path).

ğŸ”¹ Ingress + Ingress Controller:
	â€¢	You define an Ingress resource, which:
	â€¢	Specifies which service it targets (e.g., frontend).
	â€¢	Defines host-based and/or path-based routing (e.g., only allow amazon.com/xyz).
	â€¢	The Ingress Controller (a running component in the cluster):
	â€¢	Reads the Ingress YAML file.
	â€¢	Creates and configures a load balancer based on the routing rules.
	â€¢	Acts like CCM, but offers more flexibility in how traffic is routed.

â¸»

ğŸ“˜ Example:

A sample Ingress resource may define:

spec:
  rules:
    - host: amazon.com
      http:
        paths:
          - path: /xyz
            pathType: Prefix
            backend:
              service:
                name: frontend
                port:
                  number: 8080

This means:
	â€¢	Requests to amazon.com/xyz are routed to the frontend service on port 8080.

â¸»

ğŸ’¡ Key Points:
	â€¢	Ingress is needed only when you want to expose a service externally with specific rules.
	â€¢	You donâ€™t need Ingress for internal services like backend or email services.
	â€¢	In a real-world setup, typically only the frontend (or API gateway) gets an Ingress resource.
	â€¢	Ingress resource does not create a load balancer by itself â€” the Ingress Controller reads it and provisions one.

â¸»

ğŸ“Œ Interview Tip:

If asked â€œWhen do you create an Ingress?â€:

â€œYou create an Ingress only when you need to expose a service externally and want to define domain- or path-based routing rules. Typically, only the frontend service gets an Ingress resource.â€

CLASS 12
Understanding Ingress and Ingress Controllers Part 2


ğŸ“Œ What Is an Ingress Controller?
	â€¢	An Ingress Controller is a Kubernetes controller that:
	â€¢	Watches for Ingress resources in the cluster.
	â€¢	Reads routing rules defined in them.
	â€¢	Creates and configures a load balancer accordingly.

â¸»

â— Important Note:

Ingress Controller is not included by default in Kubernetes.

â¸»

ğŸ§© Why Not Included by Default?
	â€¢	Kubernetes is not opinionated about which load balancer to use.
	â€¢	It gives you the freedom to choose based on your needs.

â¸»

âš™ï¸ Examples of Ingress Controllers:

Load Balancer You Want	Ingress Controller to Use
NGINX	NGINX Ingress Controller
AWS ALB	AWS ALB Ingress Controller
Traefik	Traefik Ingress Controller
Kong	Kong Ingress Controller
F5	F5 Ingress Controller

	â€¢	These controllers are typically open-source Go programs, maintained by the respective vendors.

â¸»

ğŸ” How It Works:
	1.	You deploy a specific Ingress Controller (e.g. ALB Controller).
	2.	You write an Ingress Resource for your service (e.g. frontend).
	3.	The Ingress Controller reads that Ingress Resource YAML.
	4.	It provisions a Load Balancer (e.g. AWS ALB) with the specified rules (like domain or path restrictions).
	5.	The Load Balancer only allows traffic based on those rules.

â¸»

ğŸ›  Project Plan (as per the course):
	â€¢	Step 1: Deploy the AWS ALB Ingress Controller.
	â€¢	Step 2: Create the Ingress Resource for the frontend service.
	â€¢	Step 3: Access the service via the domain name defined in the Ingress resource (not via random IP/DNS).

â¸»

ğŸ’¬ Key Takeaway:

Deploying an Ingress resource alone does nothing.
You must have an Ingress Controller running to interpret it and create the corresponding Load Balancer.


Class 13

REPO
https://github.com/iam-veeramalla/ultimate-devops-project-aws/blob/main/14-kubernetes-ingress-controller.md


ğŸ“Œ Goal:

Install the ALB (Application Load Balancer) Ingress Controller in an EKS cluster to route external traffic based on Ingress rules.

â¸»

ğŸ§  Key Concept: IAM OIDC Provider
	â€¢	Problem:
The ALB Controller (which runs as a Pod inside your EKS cluster) needs permission to create AWS resources (like Load Balancers), which are outside the Kubernetes cluster.
	â€¢	Solution:
Use IAM Roles for Service Accounts (IRSA):
	â€¢	Create an IAM Role with necessary permissions.
	â€¢	Bind this IAM Role to the Kubernetes Service Account used by the ALB Controller.
	â€¢	Use an OIDC (OpenID Connect) Provider to securely associate them.
	â€¢	This setup allows the ALB Controller pod to act with AWS IAM permissions without hardcoding credentials.

â¸»

ğŸªœ Installation Steps (High-Level):

1. ğŸ“‚ Check Documentation
	â€¢	Refer to the official project docs:
kubernetes-ingress-controller.md
(File name may vary as it gets updated frequently)

2. ğŸ“› Export Cluster Name

export CLUSTER_NAME=my-eks-cluster

3. ğŸ” Fetch OIDC ID for the Cluster

OIDC_ID=$(aws eks describe-cluster \
  --name $CLUSTER_NAME \
  --query "cluster.identity.oidc.issuer" \
  --output text | cut -d '/' -f 3)

4. ğŸ”— Associate IAM OIDC Provider to the Cluster

eksctl utils associate-iam-oidc-provider \
  --cluster $CLUSTER_NAME \
  --approve

This step allows EKS service accounts to assume IAM roles via OIDC.

â¸»

ğŸ’¬ Interview Insight:

Q: How can a Kubernetes Pod in EKS create AWS resources like Load Balancers?
A: By using IAM Roles for Service Accounts (IRSA) connected via IAM OIDC provider, which lets the pod securely assume an IAM Role with appropriate policies.

â¸»

âœ… Next Steps in the Course:
	1.	Install the ALB Ingress Controller.
	2.	Create Ingress Resource for the frontend service.
	3.	Access the frontend using the specified domain name.

Objective:

Install the ALB Controller by:
	1.	Creating an IAM policy with ALB-related permissions.
	2.	Creating an IAM role and attaching it to the Kubernetes service account used by the ALB Controller.

â¸»

ğŸªœ Step-by-Step Summary:

1. ğŸ“„ Create IAM Policy
	â€¢	The ALB Controller requires specific permissions to manage Elastic Load Balancers.
	â€¢	These permissions are not manually written â€” they are provided by AWS.

ğŸ§° Download Policy JSON:

curl -o iam-policy.json \
https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.11.0/docs/install/iam_policy.json

	â€¢	This file contains all the required IAM permissions for ALB operations.

ğŸ“¤ Create the IAM Policy in AWS:

aws iam create-policy \
  --policy-name AWSLoadBalancerControllerIAMPolicy \
  --policy-document file://iam-policy.json

	â€¢	This uploads the policy and makes it available for attachment to IAM roles.

â¸»

2. ğŸ§‘â€ğŸ’¼ Create IAM Role and Attach to Kubernetes Service Account
	â€¢	The ALB Controller pod runs inside Kubernetes using a service account.
	â€¢	To let the pod assume AWS permissions, we bind the service account to the IAM role using OIDC.

Steps for creating the role and binding it to the service account are provided in the documentation (continuation of what comes next).

â¸»

ğŸ’¬ Interview Tip:

Q: How does the ALB Ingress Controller pod gain permission to create AWS Load Balancers?

A:
	1.	AWS provides a pre-defined IAM policy for ALB operations.
	2.	This policy is attached to an IAM role.
	3.	The Kubernetes service account for the controller is linked to this IAM role using OIDC (OpenID Connect).
	4.	This allows the pod to securely assume AWS permissions without storing credentials.


